{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "QmQMawUbNaGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk"
      ],
      "metadata": {
        "id": "P3CapHJcNZAt"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Preprocessing"
      ],
      "metadata": {
        "id": "R65EBHoJNik1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lemmatizer and download necessary NLTK data\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load the dataset\n",
        "with open('/content/first-aid-datasets-updated-2.json') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Preprocess patterns for training\n",
        "def preprocess_sentence(sentence):\n",
        "    # Tokenize, lemmatize, and remove stop words\n",
        "    tokens = sentence.split()\n",
        "    tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "patterns = []\n",
        "responses = []\n",
        "tags = []\n",
        "\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        preprocessed_pattern = preprocess_sentence(pattern)\n",
        "        patterns.append(preprocessed_pattern)\n",
        "        responses.append(intent['responses'])\n",
        "        tags.append(intent['tag'])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(patterns, tags, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNBkU_JYNj0d",
        "outputId": "4dab382d-c2e2-45ee-c02e-c878938dcc04"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "6gcjf90vNrPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the machine learning model\n",
        "model = make_pipeline(\n",
        "    TfidfVectorizer(max_features=None, ngram_range=(1, 2)),\n",
        "    LinearSVC()\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model performance with additional metrics\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.2f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.2f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred, average='weighted'):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxBf9pTgNtCt",
        "outputId": "fe47a2c1-42de-41c5-d70d-2501f1f8b1c4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.87%\n",
            "Precision: 0.94\n",
            "Recall: 0.95\n",
            "F1-Score: 0.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Logical and Probabilistic Reasoning"
      ],
      "metadata": {
        "id": "LmUiOYfpNvcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logical Reasoning Rules\n",
        "logical_rules = {\n",
        "    (\"fever\", \"headache\"): \"You might have the flu. Rest and hydrate.\",\n",
        "    (\"cut\", \"bleeding\"): \"Apply pressure to stop the bleeding and clean the wound.\"\n",
        "}\n",
        "\n",
        "def logical_reasoning(user_input_tokens):\n",
        "    \"\"\"Perform logical reasoning based on input tokens.\"\"\"\n",
        "    for rule_conditions, conclusion in logical_rules.items():\n",
        "        if all(condition in user_input_tokens for condition in rule_conditions):\n",
        "            return conclusion\n",
        "    return None\n",
        "\n",
        "# Probabilistic Reasoning\n",
        "def bayesian_reasoning(symptoms):\n",
        "    \"\"\"Perform probabilistic reasoning using a basic Bayesian model.\"\"\"\n",
        "    probabilities = {\"fever\": 0.8, \"headache\": 0.7, \"nausea\": 0.5}\n",
        "    posterior = 1\n",
        "    for symptom in symptoms:\n",
        "        posterior *= probabilities.get(symptom, 0.1)\n",
        "    return f\"Based on symptoms, there is a {posterior * 100:.2f}% chance of illness.\""
      ],
      "metadata": {
        "id": "7ZZoZs-eNvG0"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Knowledge Representation (Ontology)"
      ],
      "metadata": {
        "id": "oYiW2feAN0t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ontology = {\n",
        "    \"fever\": [],\n",
        "    \"bleeding\": [],\n",
        "    \"headache\": []\n",
        "}\n",
        "\n",
        "def semantic_understanding(user_input):\n",
        "    \"\"\"Map user input to known concepts in the ontology.\"\"\"\n",
        "    tokens = user_input.split()\n",
        "    matched_concepts = []\n",
        "    for concept, keywords in ontology.items():\n",
        "        if any(keyword in tokens for keyword in keywords):\n",
        "            matched_concepts.append(concept)\n",
        "    return matched_concepts"
      ],
      "metadata": {
        "id": "TjpKocLSN2jN"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Feedback Learning Mechanism"
      ],
      "metadata": {
        "id": "cbQK2mBBStkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_feedback_to_data(user_input, predicted_intent):\n",
        "    \"\"\"Add user feedback (new data) to the training set for future learning.\"\"\"\n",
        "    new_data = {\"patterns\": [user_input], \"tag\": predicted_intent, \"responses\": [\"Thank you for your input!\"]}\n",
        "    data['intents'].append(new_data)  # Append to existing dataset\n",
        "    patterns.append(preprocess_sentence(user_input))  # Preprocess and add pattern\n",
        "    tags.append(predicted_intent)  # Add predicted intent tag\n",
        "    # Retrain model with the updated data\n",
        "    X_train_new, _, y_train_new, _ = train_test_split(patterns, tags, test_size=0.2, random_state=42)\n",
        "    model.fit(X_train_new, y_train_new)"
      ],
      "metadata": {
        "id": "Sz12Q0maSv2Z"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot Response Logic"
      ],
      "metadata": {
        "id": "WUM9cLPgSx8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a confidence threshold\n",
        "CONFIDENCE_THRESHOLD = 0.1\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "    # Preprocess user input\n",
        "    preprocessed_input = preprocess_sentence(user_input)\n",
        "\n",
        "    # Logical reasoning\n",
        "    user_tokens = preprocessed_input.split()\n",
        "    logical_result = logical_reasoning(user_tokens)\n",
        "    if logical_result:\n",
        "        return logical_result\n",
        "\n",
        "    # Semantic understanding\n",
        "    matched_concepts = semantic_understanding(preprocessed_input)\n",
        "    if matched_concepts:\n",
        "        return bayesian_reasoning(matched_concepts)\n",
        "\n",
        "    # Machine learning-based intent prediction\n",
        "    vectorizer = model.named_steps['tfidfvectorizer']\n",
        "    classifier = model.named_steps['linearsvc']\n",
        "    input_vector = vectorizer.transform([preprocessed_input])\n",
        "    decision_values = classifier.decision_function(input_vector)\n",
        "\n",
        "    # Determine intent with the highest confidence\n",
        "    intent_index = decision_values.argmax()\n",
        "    confidence = decision_values[0][intent_index]\n",
        "    predicted_intent = classifier.classes_[intent_index]\n",
        "\n",
        "    # Respond based on confidence\n",
        "    if confidence >= CONFIDENCE_THRESHOLD:\n",
        "        for intent_data in data['intents']:\n",
        "            if intent_data['tag'] == predicted_intent:\n",
        "                return random.choice(intent_data['responses'])\n",
        "    else:\n",
        "        # If the confidence is low, ask for feedback and add it to the training data\n",
        "        add_feedback_to_data(user_input, predicted_intent)\n",
        "        return \"I'm sorry, I couldn't understand that. Could you rephrase?\""
      ],
      "metadata": {
        "id": "W8cf5DhtS1l5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Loop"
      ],
      "metadata": {
        "id": "nWjdP9t8S4kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hello! I am your first aid assistant. Type 'quit' to exit.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    response = chatbot_response(user_input)\n",
        "    print(f\"Bot: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUe3nkndS6C4",
        "outputId": "625aab2c-b6a4-4d26-d99b-de85d076b161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I am your first aid assistant. Type 'quit' to exit.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}